{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from functools import lru_cache\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, Ridge\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RAND = 123\n",
    "PATH_TO_DATA = 'data/'\n",
    "PATH_TO_SUBMIT = 'submissions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(PATH_TO_DATA + \"site_dic.pkl\", \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "site_by_ind = [None] * (len(site_dict) + 1)\n",
    "for site, id in site_dict.items():\n",
    "    site_by_ind[id] = site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_name = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times_name] = train_df[times_name].apply(pd.to_datetime)\n",
    "test_df[times_name] = test_df[times_name].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sites_name = ['site{}'.format(i) for i in range(1, 11)]\n",
    "train_df[sites_name] = train_df[sites_name].fillna(0).astype('int')\n",
    "test_df[sites_name] = test_df[sites_name].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(by='time1')\n",
    "test_df = test_df.sort_values(by='time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_df['target']\n",
    "train_df = train_df.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_df, small_y = train_df[4000:4500], y[4000:4500]\n",
    "sum(small_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file, index=None,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    if index is None:\n",
    "        index = np.arange(1, predicted_labels.shape[0] + 1)\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = index,\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(PATH_TO_SUBMIT + '/' + out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generete_bag_of_sites(data):\n",
    "    full_sites = data[sites_name]\n",
    "    sites_flatten = full_sites.values.flatten()\n",
    "    full_sites_sparse = csr_matrix(([1] * sites_flatten.shape[0],\n",
    "                                    sites_flatten,\n",
    "                                    range(0, sites_flatten.shape[0] + 10, 10)))[:, 1:]\n",
    "    return full_sites_sparse\n",
    "\n",
    "def generete_text_of_sites(data):\n",
    "    full_sites = data[sites_name]\n",
    "    return full_sites.apply(lambda x: '#'.join(site_by_ind[site] \n",
    "                                               for site in filter(lambda y: y, x)), axis=1)\n",
    "\n",
    "nano_in_min = 10**9 * 60\n",
    "bool_to_int = {True: 1, False: 0}\n",
    "\n",
    "def one_hot_smooth_encode(val, max_val):\n",
    "    result = np.zeros(max_val)\n",
    "    ival = int(val)\n",
    "    result[ival] += 1 - (val - ival)\n",
    "    result[(ival + 1) % max_val] += (val - ival)\n",
    "    return pd.Series(result)\n",
    "\n",
    "def encode_time(data, smooth=3, max_time=24, prefix=''):\n",
    "    max_val = (max_time + smooth - 1) // smooth\n",
    "    \n",
    "    def encode(x):\n",
    "        return one_hot_smooth_encode(x / smooth, max_val)\n",
    "    \n",
    "    result = data.apply(encode)\n",
    "    result = result.rename(columns={ind: prefix + str(ind) \n",
    "                                    for ind in range(max_val)})\n",
    "    return result\n",
    "\n",
    "def generate_cat_time_features(data):\n",
    "    full_times = data[times_name]\n",
    "    start_time = full_times['time1']\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    hours = start_time.apply(lambda x: x.hour + x.minute / 60)\n",
    "    \n",
    "    features['is_morning'] = ((hours > 6) & (hours <= 11)).map(bool_to_int)\n",
    "    features['is_day'] = ((hours > 11) & (hours <= 17)).map(bool_to_int)\n",
    "    features['is_evning'] = ((hours > 17) & (hours <= 24)).map(bool_to_int)\n",
    "    features['is_nignt'] = ((hours >= 0) & (hours <= 6)).map(bool_to_int)\n",
    "    \n",
    "    smooth_hours = 1\n",
    "    features = pd.concat([features, \n",
    "                          encode_time(hours, \n",
    "                                      prefix='smooth{}_hours_'.format(smooth_hours), \n",
    "                                      smooth=smooth_hours)], axis=1)\n",
    "     \n",
    "    #month = full_times['time1'].apply(lambda x: x.month)\n",
    "    #features = pd.concat([features, \n",
    "    #                      encode_time(month - 1, prefix='smooth2_month_', smooth=4, max_time=12)], axis=1)\n",
    "    \n",
    "    dayofweek = pd.get_dummies(start_time.apply(lambda x: x.dayofweek), prefix='day')\n",
    "    features = pd.concat([features, dayofweek], axis=1)\n",
    "    \n",
    "    features['is_day_off'] = (5 <= start_time.apply(lambda x: x.dayofweek)).map(bool_to_int)\n",
    "    \n",
    "    features.DESCRIPTION = ' '.join(features.columns)\n",
    "    return features\n",
    "\n",
    "def generate_count_time_features(data):\n",
    "    full_times = data[times_name]\n",
    "    start_time = full_times['time1']\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "\n",
    "    features['month'] = start_time.apply(lambda x: x.month)\n",
    "    #features['year'] = start_time.apply(lambda x: x.year + x.month / 12)\n",
    "    features['trand_10000_100_1'] = start_time.apply(lambda x: x.year * 10000 + x.month * 100 + x.day)\n",
    "    #features['trand_log_10000_100_1'] = start_time.apply(lambda x: np.log(x.year * 10000 + x.month * 100 + x.day))\n",
    "    #features['trand_1000_100_1'] = start_time.apply(lambda x: x.year * 1000 + x.month * 100 + x.day)\n",
    "    \n",
    "    seconds_from_start = train_df[times_name].\\\n",
    "        applymap(lambda x: np.nan if pd.isnull(x) else x.value)\n",
    "    features['duration'] = (seconds_from_start.max(1) - seconds_from_start.min(1)) / nano_in_min\n",
    "    #features['sites_count'] = (~full_times.isnull()).sum(1)\n",
    "    #features['mean_duration'] = features['duration'] / features['sites_count']\n",
    "    \n",
    "    features.DESCRIPTION = ' '.join(features.columns)\n",
    "    return features\n",
    "\n",
    "def generate_y_prob(text, y, vocab, alpha=10):\n",
    "    n = len(vocab)\n",
    "    good_count = np.zeros(n + 1)\n",
    "    count = np.zeros(n + 1)\n",
    "    for session, is_good in zip(text, y):\n",
    "        for site in session.split('#'):\n",
    "            ind = vocab.get(site, n)\n",
    "            count[ind] += 1\n",
    "            if is_good == 1:\n",
    "                good_count[ind] += 1\n",
    "    global_mean = good_count.sum() / count.sum()\n",
    "    prob = (good_count + global_mean * alpha) / (count + alpha)\n",
    "    return csr_matrix([prob[:-1]])\n",
    "\n",
    "def generate_all_features(data, y=None, transformers=None, X_test=None):\n",
    "    description = ''\n",
    "    X_cat_time = generate_cat_time_features(data)\n",
    "    X_count_time = generate_count_time_features(data)\n",
    "    X_text = generete_text_of_sites(data)\n",
    "    \n",
    "    description += 'Features_cat_time: {}\\n'.format(X_cat_time.DESCRIPTION)\n",
    "    description += 'Features_count_time: {}\\n'.format(X_count_time.DESCRIPTION)\n",
    "    \n",
    "    if not transformers:\n",
    "        transformers = {}\n",
    "    if 'vecorizer' not in transformers:\n",
    "        #transformers['vecorizer'] = TfidfVectorizer(max_features=10000,\n",
    "        #                                            analyzer='word', \n",
    "        #                                            token_pattern='[^#]+').fit(X_text)\n",
    "        #vocab = transformers['vecorizer'].vocabulary_\n",
    "        #transformers['y_prob'] = generate_y_prob(X_text, y, vocab)\n",
    "        \n",
    "        if X_test is not None:\n",
    "            text_test = generete_text_of_sites(X_test)\n",
    "            all_text = pd.concat((X_text, text_test))\n",
    "        else:\n",
    "            all_text = X_text\n",
    "        transformers['vecorizer'] = TfidfVectorizer(max_features=8500,\n",
    "                                                    analyzer='word', \n",
    "                                                    token_pattern='[^#]+').fit(all_text)\n",
    "    X_text_csr = transformers['vecorizer'].transform(X_text)\n",
    "    description += '{}: {}\\n'.format('vecorizer Train+Test', transformers['vecorizer'])\n",
    "    \n",
    "    \n",
    "    if 'TfidfTransformer' not in transformers:\n",
    "        if X_test is not None:\n",
    "            cat_test = generate_cat_time_features(X_test)\n",
    "            all_cat = pd.concat((X_cat_time, cat_test))\n",
    "        else:\n",
    "            all_cat = X_cat_time\n",
    "        transformers['TfidfTransformer'] = TfidfTransformer().fit(X_cat_time)  \n",
    "    X_cat_time_csr = csr_matrix(transformers['TfidfTransformer'].transform(X_cat_time))\n",
    "    description += '{}: {}\\n'.format('transformer X_cat_time', transformers['TfidfTransformer'])\n",
    "    \n",
    "    \n",
    "    if 'scaler' not in transformers:\n",
    "        transformers['scaler'] = StandardScaler().fit(X_count_time)\n",
    "    X_count_time_csr = csr_matrix(\n",
    "        transformers['scaler'].transform(X_count_time))\n",
    "    description += '{}: {}\\n'.format('scaler for count', transformers['scaler'])\n",
    "\n",
    "    #X_text_csr = X_text_csr.multiply(transformers['y_prob'])\n",
    "    #description += 'y_prob\\n'\n",
    "    \n",
    "    result = hstack((X_cat_time_csr, X_count_time_csr, X_text_csr)).tocsc()\n",
    "    \n",
    "    result.DESCRIPTION = description\n",
    "    result.ANOTATION = X_cat_time.columns.tolist()\n",
    "    result.ANOTATION += X_count_time.columns.tolist()\n",
    "    return result, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.5\n",
    "    \n",
    "\n",
    "def log(file_name='temp_results.txt', last_action_file_name='last_action.txt',\n",
    "        X=None, model=None, score=None, status=None):\n",
    "    log_str = 'Status: {}\\n'.format(status)\n",
    "    if score is not None:\n",
    "        log_str += 'Score = {}\\n\\n'.format(score)\n",
    "    if model is not None:\n",
    "        log_str += 'Model:\\n{}\\n\\n'.format(model)\n",
    "    if X is not None:\n",
    "        log_str += 'Features:\\n{}\\n'.format(X.DESCRIPTION)\n",
    "    \n",
    "    with open(file_name, 'a') as fl:\n",
    "        fl.write('-'*100 + '\\n');\n",
    "        fl.write(log_str)\n",
    "    \n",
    "    with open(last_action_file_name, 'w') as fl:\n",
    "        fl.write(log_str)\n",
    "        \n",
    "    return log_str\n",
    "\n",
    "\n",
    "def train_test_features(X_train, y_train, X_test):\n",
    "    X_train, transformers = generate_all_features(X_train, y=y_train, X_test=X_test)\n",
    "    X_test, transformers = generate_all_features(X_test, transformers=transformers)\n",
    "    return X_train, X_test, transformers\n",
    "\n",
    "\n",
    "def train_test_features_split(X, y, return_transformers=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=TEST_SIZE)\n",
    "    X_train, X_test, transformers = train_test_features(X_train, y_train, X_test)\n",
    "    if return_transformers:\n",
    "        return X_train, X_test, y_train, y_test, transformers\n",
    "    else:\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def generate_substr_featuresd(window_ans):\n",
    "    return [np.max(window_ans),\n",
    "            np.percentile(window_ans, 0.9),\n",
    "            np.percentile(window_ans, 0.7),\n",
    "            np.mean(window_ans)]\n",
    "\n",
    "def generate_substr_features(window_ans, cur_ans):\n",
    "    n = len(window_ans)\n",
    "    window_ans[len(window_ans) // 2] = 0\n",
    "    return [cur_ans] + generate_substr_featuresd(window_ans)\n",
    "\n",
    "def predict_model2(model1, model2, l_model, X, y, X_test, n_win=3):\n",
    "    n_train = len(y)\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    train_predict = []\n",
    "    \n",
    "    for i in range(n_win):\n",
    "        l = i * (n_train//n_win)\n",
    "        r = (i + 1) * (n_train//n_win)\n",
    "        train_X_loc = vstack((X[:l], X[r:]))\n",
    "        train_y_loc = np.concatenate((y[:l], y[r:]))\n",
    "        test_X = X[l:r]\n",
    "        model1.fit(train_X_loc, train_y_loc)\n",
    "        loc_predict = model.predict_proba(test_X)[:,1]\n",
    "        train_predict = np.concatenate((train_predict, loc_predict))\n",
    "    \n",
    "    substr_X = [generate_substr_features(train_predict[i-l_model:i+l_model], train_predict[i])\n",
    "                for i in range(l_model, n_train-l_model)]\n",
    "    substr_y = y[l_model:n_train-l_model]\n",
    "    \n",
    "    model2.fit(substr_X, substr_y)\n",
    "    model1.fit(X, y)\n",
    "    predict = model1.predict_proba(X_test)[:,1]\n",
    "    predict_2 = predict\n",
    "    \n",
    "    for i in range(l_model, n_test-l_model):\n",
    "        if i >= l_model and i < n_test - l_model:\n",
    "            features = generate_substr_features(predict[i-l_model:i+l_model], predict[i])\n",
    "            pred = model2.predict_proba([features])[0][1]\n",
    "            predict_2[i] = (predict_2[i] + pred) / 2\n",
    "            predict_2[i] = max(min(predict_2[i], 1), 0)\n",
    "    \n",
    "    return predict_2\n",
    "\n",
    "def score2(model, model2, l, X, y, X_test, y_test, write_log=True):\n",
    "    predict = predict_model2(model, model2, l, X, y, X_test)\n",
    "            \n",
    "    auc_score = roc_auc_score(y_test, predict)\n",
    "    \n",
    "    if write_log:\n",
    "        log(model=(model, model2), score=auc_score, status='score two models', X=X)\n",
    "        \n",
    "    return auc_score\n",
    "\n",
    "\n",
    "def cross_score2(model, model2, l_model, X, y, n_splits=8, n_test=3, write_log=True):\n",
    "    l = len(y) // n_splits\n",
    "    auc_score = np.array([score2(model, model2, l_model,\n",
    "                                 X[i*l:(i+n_test)*l], y[i*l:(i+n_test)*l],\n",
    "                                 X[(i+n_test+1)*l:], y[(i+n_test+1)*l:],\n",
    "                                 write_log=False)\n",
    "                          for i in range(n_splits - n_test - 1)])\n",
    "    if write_log:\n",
    "        log(model=(model, model2), \n",
    "            score=auc_score, \n",
    "            status='score two models with my fold: {}'.format((n_splits, n_test)), \n",
    "            X=X)\n",
    "    return auc_score\n",
    "\n",
    "\n",
    "def submit2(model, model2, l_model, X, y, X_test, name='twomodels_submit'):\n",
    "    for ind in range(1, 1000):\n",
    "        name_with_ind = name + '_' + str(ind)\n",
    "        if name_with_ind not in os.listdir(PATH_TO_SUBMIT):\n",
    "            name = name_with_ind\n",
    "            break\n",
    "    print(name)\n",
    "    \n",
    "    index = X_test.index\n",
    "    X, X_test, transformers = train_test_features(X, y, X_test)\n",
    "    predict = predict_model2(model, model2, l_model, X, y, X_test)\n",
    "    \n",
    "    log(X=X, model=model, status='submit two models {}'.format(name))\n",
    "    write_to_submission_file(predict, name, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X, transformers = generate_all_features(train_df, y=y, X_test=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82394868452\n",
      "[ 0.5644702   0.9424009   0.96497495]\n",
      "Diff with last: -0.028538441171195417\n",
      "Diff with best: -0.13200106720928484\n",
      "CPU times: user 2min 48s, sys: 6.42 s, total: 2min 55s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(C=5, random_state=RAND)\n",
    "model2 = LogisticRegression(C=5, random_state=RAND)\n",
    "#model2 = Ridge(random_state=RAND)\n",
    "#model2 = GradientBoostingClassifier(n_estimators=10)\n",
    "#model2 = GradientBoostingRegressor(n_estimators=20)\n",
    "arr_score = cross_score2(model, model2, 10, X, y, n_splits=7, n_test=3)\n",
    "new_score = arr_score.mean()\n",
    "print(new_score)\n",
    "print(arr_score)\n",
    "\n",
    "print('Diff with last: {}'.format(new_score - last_score))\n",
    "print('Diff with best: {}'.format(new_score - last_best_score))\n",
    "last_score = new_score\n",
    "if last_best_score < new_score:\n",
    "    last_best_score = new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol2_2\n",
      "CPU times: user 3min 48s, sys: 6.96 s, total: 3min 55s\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submit2(model, model2, 10, train_df, y, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 8539\n",
      "  -4.69  4489 trand_10000_100_1\n",
      "  -4.43  4616 smooth1_hours_11\n",
      "   4.27  4657 smooth1_hours_18\n",
      "  -4.12  4560 day_4\n",
      "  -3.97  4488 month\n",
      "  -3.84  4563 day_2\n",
      "  -3.71  4269 is_morning\n",
      "   2.54  4648 smooth1_hours_17\n",
      "   2.41  4679 day_0\n",
      "  -2.09  4600 smooth1_hours_9\n",
      "   1.60  4662 smooth1_hours_19\n",
      "   1.53  4623 smooth1_hours_13\n",
      "   1.36  4692 day_1\n",
      "  -1.33  4636 smooth1_hours_14\n",
      "  -1.29  4608 smooth1_hours_10\n",
      "  -1.27  4645 smooth1_hours_15\n",
      "   1.24  4695 day_3\n",
      "   1.22  4646 smooth1_hours_16\n",
      "  -1.13  4599 smooth1_hours_8\n",
      "  -0.88  4473 is_day_off\n",
      "  -0.88  4547 day_5\n",
      "   0.64  4564 is_day\n",
      "  -0.45  4665 smooth1_hours_20\n",
      "  -0.35  4494 duration\n",
      "  -0.30  4667 smooth1_hours_22\n",
      "   0.26  4619 smooth1_hours_12\n",
      "  -0.22  4668 smooth1_hours_23\n",
      "  -0.17  4666 smooth1_hours_21\n",
      "  -0.13  4472 day_6\n",
      "  -0.04  4575 smooth1_hours_0\n",
      "   0.00  4569 is_evning\n",
      "  -0.00  4596 smooth1_hours_7\n",
      "   0.00  4591 smooth1_hours_6\n",
      "   0.00  4587 smooth1_hours_5\n",
      "   0.00  4585 smooth1_hours_4\n",
      "   0.00  4583 smooth1_hours_3\n",
      "   0.00  4582 smooth1_hours_2\n",
      "   0.00  4579 smooth1_hours_1\n",
      "   0.00  4571 is_nignt\n"
     ]
    }
   ],
   "source": [
    "new_features = X.ANOTATION\n",
    "count_features = len(new_features)\n",
    "if type(model) is LogisticRegression:\n",
    "    coef = model.coef_[0]\n",
    "else:\n",
    "    coef = np.array([m.coef_[0] for m in model.estimators_]).mean(0)\n",
    "argsort_coef = np.argsort(np.abs(coef))\n",
    "argsort_features = np.argsort(np.abs(coef[:count_features]))[::-1]\n",
    "\n",
    "print('Total', len(coef))\n",
    "for ind_feature in argsort_features:\n",
    "    name = new_features[ind_feature]\n",
    "    value = coef[ind_feature]\n",
    "    num = argsort_coef[ind_feature]\n",
    "    print('{:7.2f} {:5} {}'.format(value, num, name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
